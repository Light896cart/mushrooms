{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Optional, Tuple, Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMushroom(Dataset):\n",
    "    def __init__(self, file_csv_x, file_csv_y=None):\n",
    "        \"\"\"\n",
    "        Инициализация датасета грибов\n",
    "        Args:\n",
    "            file_csv_x: DataFrame с входными признаками\n",
    "            file_csv_y: DataFrame с целевыми переменными (опционально)\n",
    "        \"\"\"\n",
    "        self.file_csv_x = file_csv_x.copy().astype(int, errors='ignore') \n",
    "        self.file_csv_y = file_csv_y.copy().astype(int, errors='ignore') if file_csv_y is not None else pd.DataFrame()\n",
    "        self.encoders_x = {}\n",
    "        self.encoders_y = {}\n",
    "\n",
    "        print('СУПЕР ОСНОВНОЙ X')\n",
    "        print(self.file_csv_x)\n",
    "        print('СУПЕР ОСНОВНОЙ Y')\n",
    "        print(self.file_csv_y)\n",
    "        \n",
    "        # Сохраняем служебные столбцы отдельно\n",
    "        self.observation_ids = self.file_csv_x['observationID'].copy()\n",
    "        self.filenames = self.file_csv_x['filename'].copy()\n",
    "        \n",
    "        # Удаляем служебные столбцы перед обработкой\n",
    "        self.file_csv_x = self.file_csv_x.drop(['observationID', 'filename'], axis=1)\n",
    "        \n",
    "        # Сохраняем исходные метки до токенизации\n",
    "        if not self.file_csv_y.empty:\n",
    "            self.original_labels = self.file_csv_y.copy()\n",
    "            # Сохраняем category_id отдельно\n",
    "            self.category_ids = self.file_csv_y['category_id'].copy()\n",
    "            # Удаляем category_id перед токенизацией\n",
    "            self.file_csv_y = self.file_csv_y.drop(['category_id'], axis=1)\n",
    "        \n",
    "        self._preprocess_data()\n",
    "        \n",
    "        print('ПОСЛЕ ОБРАБОТКИ X')\n",
    "        print(self.token_x)\n",
    "        print('ПОСЛЕ ОБРАБОТКИ Y')\n",
    "        print(self.token_y)\n",
    "        \n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"Предобработка данных: токенизация и заполнение пропусков\"\"\"\n",
    "        self.token_x, self.token_y = self._tokenize_features()\n",
    "        print('ОСНОВНОЙ X')\n",
    "        print(self.token_x)\n",
    "        print('ОСНОВНОЙ Y')\n",
    "        print(self.token_y)\n",
    "        self.token_x, self.token_y = self._fill_missing_values()\n",
    "        print('НЕ ОСНОВНОЙ X')\n",
    "        print(self.token_x)\n",
    "        print('НЕ ОСНОВНОЙ Y')\n",
    "        print(self.token_y)\n",
    "        \n",
    "    def _tokenize_features(self):\n",
    "        \"\"\"Токенизация категориальных признаков\"\"\"\n",
    "        # Токенизация X\n",
    "        for column in self.file_csv_x.select_dtypes(include=['object']).columns:\n",
    "            self.encoders_x[column] = LabelEncoder()\n",
    "            self.file_csv_x[column] = self.encoders_x[column].fit_transform(\n",
    "                self.file_csv_x[column].astype(str)\n",
    "            )\n",
    "            \n",
    "        # Токенизация y (кроме category_id)\n",
    "        if not self.file_csv_y.empty:\n",
    "            for column in self.file_csv_y.columns:\n",
    "                self.encoders_y[column] = LabelEncoder()\n",
    "                self.file_csv_y[column] = self.encoders_y[column].fit_transform(\n",
    "                    self.file_csv_y[column].astype(str)\n",
    "                )\n",
    "        \n",
    "        return self.file_csv_x, self.file_csv_y\n",
    "    \n",
    "    def _fill_missing_values(self):\n",
    "        \"\"\"Заполнение пропущенных значений с помощью RandomForest\"\"\"\n",
    "        def fill_df_missing_values(df):\n",
    "            for column in df.columns:\n",
    "                if df[column].isnull().any():\n",
    "                    mask = df[column].isnull()\n",
    "                    non_null_data = df[~mask]\n",
    "                    \n",
    "                    if len(non_null_data) > 0:\n",
    "                        features = non_null_data.drop(column, axis=1)\n",
    "                        target = non_null_data[column]\n",
    "                        \n",
    "                        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "                        rf.fit(features, target)\n",
    "                        \n",
    "                        missing_data = df.loc[mask].drop(column, axis=1)\n",
    "                        df.loc[mask, column] = rf.predict(missing_data)\n",
    "            return df\n",
    "\n",
    "        self.token_x = fill_df_missing_values(self.token_x)\n",
    "        if not self.token_y.empty:\n",
    "            self.token_y = fill_df_missing_values(self.token_y)\n",
    "            \n",
    "        return self.token_x, self.token_y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.observation_ids.unique())\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Получение элемента датасета по индексу\n",
    "        Args:\n",
    "            idx: индекс элемента\n",
    "        Returns:\n",
    "            dict: словарь с данными элемента\n",
    "        \"\"\"\n",
    "        # Получаем все уникальные observation_ids и их индексы\n",
    "        unique_obs_ids = self.observation_ids.unique()\n",
    "        \n",
    "        # Получаем все строки данных для этого observation_id\n",
    "        obs_mask = self.observation_ids == unique_obs_ids[idx]\n",
    "        x_data = self.token_x[obs_mask]\n",
    "        \n",
    "        # Получаем все связанные изображения\n",
    "        patch_filenames = self.filenames[obs_mask].values\n",
    "        \n",
    "        # Списки для хранения данных\n",
    "        images_pil = []\n",
    "        images_tensor = []\n",
    "        \n",
    "        # Загружаем все изображения для данного observation_id\n",
    "        transform = transforms.ToTensor()\n",
    "        for filename in patch_filenames:\n",
    "            img = Image.open(f'E:/jupyter/Грибы/images/images/train/300p/{filename}')\n",
    "            images_pil.append(img)\n",
    "            images_tensor.append(transform(img))\n",
    "        \n",
    "        # Преобразуем x_data в матрицу нужной формы\n",
    "        num_features = x_data.shape[1]  # Количество признаков\n",
    "        x_tensor = torch.tensor(x_data.values, dtype=torch.float32).reshape(-1, num_features)\n",
    "        \n",
    "        sample = {\n",
    "            'x_data': x_tensor,\n",
    "            'filenames': patch_filenames.tolist(),\n",
    "            'observation_id': unique_obs_ids[idx],\n",
    "            'images_pil': images_pil,\n",
    "            'images_tensor': images_tensor\n",
    "        }\n",
    "        \n",
    "        if not self.token_y.empty:\n",
    "            # Получаем токенизированные метки и category_id\n",
    "            y_data = self.token_y[obs_mask].values\n",
    "            category_id = self.category_ids[obs_mask].values\n",
    "            # Объединяем их\n",
    "            y_combined = np.hstack([category_id.reshape(-1, 1), y_data])\n",
    "            y_tensor = torch.tensor(y_combined, dtype=torch.int64)\n",
    "            sample['y_data'] = y_tensor\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "СУПЕР ОСНОВНОЙ X\n",
      "                                                habitat  \\\n",
      "0     Mixed woodland (with coniferous and deciduous ...   \n",
      "1                                                garden   \n",
      "2                                                garden   \n",
      "3                                                garden   \n",
      "4                                                garden   \n",
      "...                                                 ...   \n",
      "7814                                 Deciduous woodland   \n",
      "7815                                 Deciduous woodland   \n",
      "7816                       Unmanaged deciduous woodland   \n",
      "7817                                        salt meadow   \n",
      "7818                                        salt meadow   \n",
      "\n",
      "                       substrate  elevation  landcover biogeographicalRegion  \\\n",
      "0     dead wood (including bark)        0.0       16.0           continental   \n",
      "1     dead wood (including bark)        0.0       17.0           continental   \n",
      "2     dead wood (including bark)        0.0       17.0           continental   \n",
      "3     dead wood (including bark)        0.0       17.0           continental   \n",
      "4     dead wood (including bark)        0.0       17.0           continental   \n",
      "...                          ...        ...        ...                   ...   \n",
      "7814   stems of herbs, grass etc       93.0        1.0           continental   \n",
      "7815   stems of herbs, grass etc       93.0        1.0           continental   \n",
      "7816  dead wood (including bark)        0.0       17.0           continental   \n",
      "7817                     insects        0.0       17.0           continental   \n",
      "7818                     insects        0.0       17.0           continental   \n",
      "\n",
      "      observationID          filename  \n",
      "0        3052832307  0-3052832307.JPG  \n",
      "1        3061954303  0-3061954303.JPG  \n",
      "2        3061954303  1-3061954303.JPG  \n",
      "3        3061954303  2-3061954303.JPG  \n",
      "4        3061954303  3-3061954303.JPG  \n",
      "...             ...               ...  \n",
      "7814     4100093368  0-4100093368.JPG  \n",
      "7815     4100093368  2-4100093368.JPG  \n",
      "7816     3429079314  2-3429079314.JPG  \n",
      "7817     4847339663  0-4847339663.JPG  \n",
      "7818     4847339663  1-4847339663.JPG  \n",
      "\n",
      "[7819 rows x 7 columns]\n",
      "СУПЕР ОСНОВНОЙ Y\n",
      "      category_id   kingdom         phylum               class\n",
      "0            2421     Fungi     Ascomycota      Eurotiomycetes\n",
      "1             386  Protozoa      Mycetozoa         Myxomycetes\n",
      "2             386  Protozoa      Mycetozoa         Myxomycetes\n",
      "3             386  Protozoa      Mycetozoa         Myxomycetes\n",
      "4             386  Protozoa      Mycetozoa         Myxomycetes\n",
      "...           ...       ...            ...                 ...\n",
      "7814         1736     Fungi     Ascomycota     Sordariomycetes\n",
      "7815         1736     Fungi     Ascomycota     Sordariomycetes\n",
      "7816         1465     Fungi  Basidiomycota      Agaricomycetes\n",
      "7817         1166     Fungi     Ascomycota  Laboulbeniomycetes\n",
      "7818         1166     Fungi     Ascomycota  Laboulbeniomycetes\n",
      "\n",
      "[7819 rows x 4 columns]\n",
      "ОСНОВНОЙ X\n",
      "      habitat  substrate  elevation  landcover  biogeographicalRegion\n",
      "0           4          6        0.0       16.0                      4\n",
      "1          15          6        0.0       17.0                      4\n",
      "2          15          6        0.0       17.0                      4\n",
      "3          15          6        0.0       17.0                      4\n",
      "4          15          6        0.0       17.0                      4\n",
      "...       ...        ...        ...        ...                    ...\n",
      "7814        2         27       93.0        1.0                      4\n",
      "7815        2         27       93.0        1.0                      4\n",
      "7816        7          6        0.0       17.0                      4\n",
      "7817       29         11        0.0       17.0                      4\n",
      "7818       29         11        0.0       17.0                      4\n",
      "\n",
      "[7819 rows x 5 columns]\n",
      "ОСНОВНОЙ Y\n",
      "      kingdom  phylum  class\n",
      "0           1       1     11\n",
      "1           2       8     21\n",
      "2           2       8     21\n",
      "3           2       8     21\n",
      "4           2       8     21\n",
      "...       ...     ...    ...\n",
      "7814        1       1     27\n",
      "7815        1       1     27\n",
      "7816        1       2      0\n",
      "7817        1       1     15\n",
      "7818        1       1     15\n",
      "\n",
      "[7819 rows x 3 columns]\n",
      "НЕ ОСНОВНОЙ X\n",
      "      habitat  substrate  elevation  landcover  biogeographicalRegion\n",
      "0           4          6        0.0       16.0                      4\n",
      "1          15          6        0.0       17.0                      4\n",
      "2          15          6        0.0       17.0                      4\n",
      "3          15          6        0.0       17.0                      4\n",
      "4          15          6        0.0       17.0                      4\n",
      "...       ...        ...        ...        ...                    ...\n",
      "7814        2         27       93.0        1.0                      4\n",
      "7815        2         27       93.0        1.0                      4\n",
      "7816        7          6        0.0       17.0                      4\n",
      "7817       29         11        0.0       17.0                      4\n",
      "7818       29         11        0.0       17.0                      4\n",
      "\n",
      "[7819 rows x 5 columns]\n",
      "НЕ ОСНОВНОЙ Y\n",
      "      kingdom  phylum  class\n",
      "0           1       1     11\n",
      "1           2       8     21\n",
      "2           2       8     21\n",
      "3           2       8     21\n",
      "4           2       8     21\n",
      "...       ...     ...    ...\n",
      "7814        1       1     27\n",
      "7815        1       1     27\n",
      "7816        1       2      0\n",
      "7817        1       1     15\n",
      "7818        1       1     15\n",
      "\n",
      "[7819 rows x 3 columns]\n",
      "ПОСЛЕ ОБРАБОТКИ X\n",
      "      habitat  substrate  elevation  landcover  biogeographicalRegion\n",
      "0           4          6        0.0       16.0                      4\n",
      "1          15          6        0.0       17.0                      4\n",
      "2          15          6        0.0       17.0                      4\n",
      "3          15          6        0.0       17.0                      4\n",
      "4          15          6        0.0       17.0                      4\n",
      "...       ...        ...        ...        ...                    ...\n",
      "7814        2         27       93.0        1.0                      4\n",
      "7815        2         27       93.0        1.0                      4\n",
      "7816        7          6        0.0       17.0                      4\n",
      "7817       29         11        0.0       17.0                      4\n",
      "7818       29         11        0.0       17.0                      4\n",
      "\n",
      "[7819 rows x 5 columns]\n",
      "ПОСЛЕ ОБРАБОТКИ Y\n",
      "      kingdom  phylum  class\n",
      "0           1       1     11\n",
      "1           2       8     21\n",
      "2           2       8     21\n",
      "3           2       8     21\n",
      "4           2       8     21\n",
      "...       ...     ...    ...\n",
      "7814        1       1     27\n",
      "7815        1       1     27\n",
      "7816        1       2      0\n",
      "7817        1       1     15\n",
      "7818        1       1     15\n",
      "\n",
      "[7819 rows x 3 columns]\n",
      "Количество пустых значений в X:\n",
      "habitat                  0\n",
      "substrate                0\n",
      "elevation                0\n",
      "landcover                0\n",
      "biogeographicalRegion    0\n",
      "dtype: int64\n",
      "\n",
      "Количество пустых значений в y:\n",
      "kingdom    0\n",
      "phylum     0\n",
      "class      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "df = pd.read_csv('E:\\jupyter\\Грибы\\metadata\\FungiTastic-FewShot\\FungiTastic-FewShot-Train.csv')\n",
    "# Выбор входных параметров X\n",
    "X = df[['habitat', 'substrate', 'elevation', 'landcover', 'biogeographicalRegion', 'observationID','filename']]\n",
    "# Выбор выходных параметров y \n",
    "y = df[['category_id', 'kingdom', 'phylum', 'class']]\n",
    "\n",
    "daset = DataMushroom(X,y)\n",
    "# Проверяем количество пустых значений в X и y\n",
    "print(\"Количество пустых значений в X:\")\n",
    "print(daset.file_csv_x.isnull().sum())\n",
    "print(\"\\nКоличество пустых значений в y:\")\n",
    "print(daset.file_csv_y.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1, 5] at entry 0 and [5, 5] at entry 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Создаем загрузчик данных\u001b[39;00m\n\u001b[0;32m      2\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(daset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m32\u001b[39m):\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID наблюдения:\u001b[39m\u001b[38;5;124m\"\u001b[39m, c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation_id\u001b[39m\u001b[38;5;124m'\u001b[39m][i])\n",
      "File \u001b[1;32mc:\\Users\\Артём\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Артём\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Артём\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Артём\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Артём\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m--> 171\u001b[0m         {\n\u001b[0;32m    172\u001b[0m             key: collate(\n\u001b[0;32m    173\u001b[0m                 [d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map\n\u001b[0;32m    174\u001b[0m             )\n\u001b[0;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[0;32m    176\u001b[0m         }\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Артём\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    171\u001b[0m         {\n\u001b[1;32m--> 172\u001b[0m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[0;32m    176\u001b[0m         }\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Артём\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\Артём\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1, 5] at entry 0 and [5, 5] at entry 3"
     ]
    }
   ],
   "source": [
    "# Создаем загрузчик данных\n",
    "train_loader = DataLoader(daset, batch_size=32, shuffle=True)\n",
    "for c in train_loader:\n",
    "    for i in range(32):\n",
    "        print(\"ID наблюдения:\", c['observation_id'][i])\n",
    "        print(\"Данные X:\", c['x_data'][i])\n",
    "        if 'y_data' in c:\n",
    "            print(\"Данные Y:\", c['y_data'][i])\n",
    "        print(\"Имена файлов:\", c['filenames'][i])\n",
    "        print(\"Тензоры изображений:\", c['images_tensor'][i])\n",
    "        print(\"Изображения:\")\n",
    "        for d in c['images_pil'][i]:\n",
    "            display(d)\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
